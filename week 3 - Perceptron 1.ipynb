{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 03 (practical 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical we will implement different versions of the Perceptron model introduced during last week lectures. As presented during lectures, the simplest Perceptron is a model that implements the following function:\n",
    "\\begin{equation*}\n",
    "y=\\Theta(w_1x_1 + w_2x_2 + \\dots + w_kx_k + b)=\\Theta(wx+b)\n",
    "\\end{equation*}\n",
    "where:\n",
    "\\begin{equation*}\n",
    "\\Theta(v)=\n",
    "  \\begin{cases}\n",
    "   1 & \\text{if } v \\geq 0 \\\\\n",
    "   0       & \\text{if } v < 0 \\end{cases}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Using perceptron to implement logical functions \n",
    "In this task you will use perceptron to implement different logical functions: NOT, AND, OR and XOR. For a logical function, both the input and the output have only two possible states: 0 and 1 (i.e., False and True). The  step binary activation function fits the purpose well since it produces a binary output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.1 Implementing the step binary activation function**\n",
    "\n",
    "In this task we implement binary step function introduced during lecture. The method takes one input. If the input value is equal or greater than zero, the method should return one, otherwise it returns zero. Note the method `step_binary` implemented below will work for both scalar and vector inputs. You can test it by passing a scalar and a numpy array as input to the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_binary(v):\n",
    "    return 1 * (v > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.2 Implementing perceptron**\n",
    "\n",
    "In this task we implement a method which performs operation of a perceptron. It takes three inputs: `x`, `w` and `b`. `x` represents the input variables, `w` represents weights of the perceptron and `b` stands for the bias. The method returns $\\Theta(wx+b)$, where $\\Theta$ is the activation function. Note that we multiply `x` and `w` using the `dot` function, rather than using `@` as it was done during lectures. This is because for some of the exercises we will need for `x` and `w` to be scalars, and `@` only works with vectors/matrices. As discussed during the lectures, we can use both `@` or `dot`, we just need to ensure that `x` and `w` are of appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(x,w,b):\n",
    "    v = np.dot(x,w)+b \n",
    "    out = step_binary(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.3 Implementing NOT logical function**\n",
    "\n",
    "NOT(x) is a 1-variable function, this means that we will have one input at a time (number of input variables $k=1$). In this task we implement a method, which takes a single value as input (we can skip validation of the input and assume that it is always correct: zero or one) and returns its negation using the perceptron method with appropriate values of the parameters `w` and `b`. Note that the main challenge of this task is finding the values of parameters `w` and `b`, which allows the perceptron to implement the NOT logical function. There are many different solutions that we could use, in this example we use $w=-1$ amd $b = 0.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NOT_(x):\n",
    "    return perceptron(x, -1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method passing $0$ or $1$ as parameter in order to evaluate whether it works correctly (i.e returns negation of the input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NOT_(0),NOT_(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.4 Implementing AND logical function**\n",
    "\n",
    "The AND logical function is a 2-variables function, $AND(x_1, x_2)$, with binary inputs and output.\n",
    "\n",
    "0 AND 1 = 0\n",
    "\n",
    "1 AND 0 = 0\n",
    "\n",
    "0 AND 0 = 0\n",
    "\n",
    "1 AND 1 = 1\n",
    "\n",
    "In this task we implement a method, which takes two values as an input and it returns the conjunction (AND) of the two boolean values. This time the perceptron is associated with the following computation: $y = \\theta(w_1x_1 + w_2x_2 + b)$. As in the previous task, we need to figure out what values of parameters $w_1$, $w_2$, and $b$ need to be used for the perceptron to behaves as AND logical operator. Let's try $w_1=1, w_2=1$ and $b=-1.5$. Hint: note that in this case we are dealing with a vector of weights rather than a single weight as in case on the NOT function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AND_(x):\n",
    "  return perceptron(x,(1,1),-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the method with the following examples in order to test whether it behaves as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([0,1])\n",
    "x2 = np.array([1,0])\n",
    "x3 = np.array([0,0])\n",
    "x4 = np.array([1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AND_(x1),AND_(x2),AND_(x3),AND_(x4),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.5 Implementing OR logical functions**\n",
    "\n",
    "Following the same idea, implement method which performs OR operation as follow:\n",
    "\n",
    "0 OR 1 = 1\n",
    "\n",
    "1 OR 0 = 1\n",
    "\n",
    "0 OR 0 = 0\n",
    "\n",
    "1 OR 1 = 1\n",
    "\n",
    "Can you guess values of the parameters that will make the perceptron operate as the OR logical function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OR_(x):\n",
    "  return perceptron(x,(1,1),-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR_(x1),OR_(x2),OR_(x3),OR_(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.6 Implementing XOR logical function**\n",
    "\n",
    "In the previous tasks you have developed three fundamental logical perceptrons: NOT, AND and OR. Now you have to build a network of those perceptrons so that it implements XOR function:\n",
    "\n",
    "0 XOR 1 = 1\n",
    "\n",
    "1 XOR 0 = 1\n",
    "\n",
    "0 XOR 0 = 0\n",
    "\n",
    "1 XOR 1 = 0\n",
    "\n",
    "The solution is as follow:\n",
    "\\begin{equation*}\n",
    "XOR(x_1,x_2) = AND(NOT(AND(x_1,x_2)),OR(x_1,x_2))\n",
    "\\end{equation*}\n",
    "\n",
    "Using the above formula, implement a method which performs XOR operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR_(x):\n",
    "  return AND_([NOT_(AND_(x)),OR_(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 0, 0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XOR_(x1),XOR_(x2),XOR_(x3),XOR_(x4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Training Perceptron with Simple Update Rule\n",
    "\n",
    "In this task we will implement a simply perceptron and train it by applying the Simple Update Rule introduced during one of our lectures. Note that training perceptron means searching for the optimal values of its parameters `w` and `b` (instead of guessing them as we did in the previous task). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T2.1 Loading dataset**\n",
    "\n",
    "1. We will first load some of the toy datasets for binary classification from the sckit-learn library. Familiarise yourself with the format of the `Breast Cancer` dataset. \n",
    "2. We will split the dataset into input and output variables.\n",
    "3. We will scale the input variable using $min$ $max$ $scaling$ method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30)\n",
      "(569,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "bc = datasets.load_breast_cancer()\n",
    "\n",
    "#extracting input and output variables\n",
    "x = bc.data\n",
    "y = bc.target\n",
    "\n",
    "#scaling the input data\n",
    "x = preprocessing.MinMaxScaler().fit_transform(x)\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T2.2 Implementing the Update Rule for training perceptron**\n",
    "\n",
    "You need to implement a method (perceptron update rule), which updates the parameters of the perceptron as per pseudo code: \n",
    "\n",
    "__Input:__ training set $x=\\{x^1, \\dots, x^n\\}, y=\\{y^1,\\dots,y^n\\}$, learning rate $\\alpha$ (set it t 0.01)\n",
    "\n",
    "1. Initialize weights $w_1,\\dots,w_k$ and $b$ with random values. Use [uniform distribution](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.random.uniform.html) to assign values between -1 and 1 to each parameter. Note that the weights vector should have the same dimension as the input and the bias is just a scalar.\n",
    "\n",
    "2. For each $x^j= \\langle x_1, \\dots, x_k\\rangle$ from x:\n",
    "\n",
    "   - Pass $x^j$ as input to the perceptron with weights $w_1,\\dots,w_k$ and bias $b$\n",
    "   - Compute the error $e$ as the difference between the expected value and the output of the perceptron $y_j-\\bar{y}^j$\n",
    "   - Adjust the parameters according to the formulas: $w_{i}^{new}=w_i+\\alpha ex^j$     $\\hspace{10mm} b^{new}=b+\\alpha e$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_perceptron(x,y,lr,iterations):\n",
    "  if len(x) != len(y):\n",
    "    return\n",
    "  weights = np.random.uniform(-1,1,len(x[1]))\n",
    "  b = np.random.uniform(-1,1)\n",
    "  for _ in range(iterations):\n",
    "    for i in range(len(x)):\n",
    "      xj = x[i]\n",
    "      ypred = perceptron(xj,weights,b)\n",
    "      e = y[i] - ypred\n",
    "      weights = weights + lr*e*xj\n",
    "      b = b + e*lr\n",
    "  return(weights, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.68741525,  0.63929525, -0.34041589,  0.36125704, -0.17808016,\n",
       "        -0.54544453,  0.45467881,  0.32495003, -0.51638239, -0.76134625,\n",
       "         0.24436284, -0.58949531,  0.87534866, -0.72868343,  0.43521091,\n",
       "        -0.05708825,  0.93623863, -0.18385619,  0.55967643,  0.96573903,\n",
       "        -0.02662122, -0.16984523, -1.12048246, -0.85189764, -0.45486489,\n",
       "        -0.31849477, -0.6116743 , -0.32155647,  0.30622635, -0.04499016]),\n",
       " 0.84758177738875)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_perceptron(x,y,0.01,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T2.3 Evaluating the perceptron model** \n",
    "\n",
    "Evaluate the perceptron before (parameters assigned with random values) and after training (parameters updated via the update rule). At this point we can just use our training data for the evaluation. Since we are dealing with a classification problem (we are predicting labels: 1 or 0) you can also use the [accuracy](https://scikit-learn.org/stable/modules/model_evaluation.html) metric (mean absolute/squared error metrics are also fine).\n",
    "\n",
    "Note that the weights are assigned randomly at the beginning, which means that you can obtain different result each time unless you set the seed of the random number generator (np.random.seed(0)). Run you program number of times to see whether the perceptron improves each time as a result of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5588752196836555\n",
      "0.7100175746924429\n"
     ]
    }
   ],
   "source": [
    "ypreds = []\n",
    "np.random.seed(0)\n",
    "weights = np.random.uniform(-1,1,len(x[1]))\n",
    "b = np.random.uniform(-1,1)\n",
    "for i in range(len(x)):\n",
    "  ypreds.append(perceptron(x[i],weights,b))\n",
    "\n",
    "print(sum(y==ypreds)/len(y))\n",
    "(weights,b) = update_perceptron(x,y,0.01,1)\n",
    "newPreds = []\n",
    "for i in range(len(x)):\n",
    "  newPreds.append(perceptron(x[i],weights,b))\n",
    "\n",
    "print(sum(y==newPreds)/len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T2.4 Increasing the time of training**\n",
    "\n",
    "As you could observe, applying the Update Rule only once to the training data set may not be enough to obtain a good model, in particular in the case of unfavorable initialization of the parameters. Increase the training of the model by applying the Update Rule multiple times to the training dataset (using for loop) to see if you can get more consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5588752196836555\n",
      "0.9701230228471002\n"
     ]
    }
   ],
   "source": [
    "ypreds = []\n",
    "np.random.seed(0)\n",
    "weights = np.random.uniform(-1,1,len(x[1]))\n",
    "b = np.random.uniform(-1,1)\n",
    "for i in range(len(x)):\n",
    "  ypreds.append(perceptron(x[i],weights,b))\n",
    "\n",
    "print(sum(y==ypreds)/len(y))\n",
    "(weights,b) = update_perceptron(x,y,0.01,50)\n",
    "newPreds = []\n",
    "for i in range(len(x)):\n",
    "  newPreds.append(perceptron(x[i],weights,b))\n",
    "\n",
    "print(sum(y==newPreds)/len(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
