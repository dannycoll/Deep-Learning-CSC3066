{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this practical you will explore how LSTM can be applied with text data in order to preform tasks such as predicting word in a sequence and predicting sentiment in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Predicting a word in a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you will train a model that for a given sequence of words passed as an input, predicts the next word in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**T1.1** Obtaining data\n",
    "\n",
    "For this task we will use the [20newsgroup dataset](http://qwone.com/~jason/20Newsgroups/) using [sklearn](https://scikit-learn.org/stable/datasets/index.html). You can load the date using the code below. Familiarize yourself with the dataset before moving to the next task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "corpus = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),categories=['sci.med'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "400\n",
      "194\n"
     ]
    }
   ],
   "source": [
    "data = corpus.data\n",
    "data_train = data[:400]\n",
    "data_test = data[400:]\n",
    "print(len(data))\n",
    "print(len(data_train))\n",
    "print(len(data_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.2** Data pre-processing\n",
    "\n",
    "Since we will be using the Embedding layer, the data should be pre-processed as in the previous practicals. In order to clean the data you can use the filter attribute to specify what characters should be remove from the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True,split=' ')\n",
    "tokenizer.fit_on_texts(data_train)\n",
    "token_list_train = tokenizer.texts_to_sequences(data_train)\n",
    "token_list_test = tokenizer.texts_to_sequences(data_test)\n",
    "num_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.3** Creating features and labels vectors \n",
    "\n",
    "The training instances will be composed of a sequence of words (we will set it to 20 for this exercise) and the labels represented by a single words.\n",
    "\n",
    "The feature and labels vectors can be generated as follows. We use the first 20 words as features with the 21st as the label, then use words 2â€“21 as features and predict the 22nd and so on. This gives us significantly more training data. \n",
    "\n",
    "Generate the features and labels vectors for the train and the test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(82696, 20)\n",
      "82696\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "train_length = 20\n",
    "\n",
    "for row in token_list_train:\n",
    "    for i in range(train_length, len(row)):\n",
    "        sequence = row[i-train_length:i+1] \n",
    "        x_train.append(sequence[:-1])\n",
    "        y_train.append(sequence[-1])\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "print(x_train.shape)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24209, 20)\n",
      "24209\n"
     ]
    }
   ],
   "source": [
    "x_test = []\n",
    "y_test = []\n",
    "train_length = 20\n",
    "\n",
    "for row in token_list_test:\n",
    "    for i in range(train_length, len(row)):\n",
    "        sequence = row[i-train_length:i+1]       \n",
    "        x_test.append(sequence[:-1])\n",
    "        y_test.append(sequence[-1])\n",
    "x_test = np.array(x_test)\n",
    "y_test = np.array(y_test)\n",
    "print(x_test.shape)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.3** Constructing the embedding weights matrix.\n",
    "\n",
    "In this task we will use the pre-trained word embeddings using the word2vec model. Create the embedding weights matrix for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import re \n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "file = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "word2vec_vectors = word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_vectors:\n",
    "        embedding_vector = word2vec[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11693, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.4** One-hot encoding the labels.\n",
    "\n",
    "Since we are dealing with a multi-class classification problem, we need to convert each label into a vector of dimension equals to the number of words. Convert the train and test labels into one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_array = np.zeros((len(y_train), num_words),dtype=int)\n",
    "for idx,word_idx in enumerate(y_train):\n",
    "    y_train_array[idx,word_idx] = 1\n",
    "    \n",
    "y_test_array = np.zeros((len(y_test), num_words),dtype=int)\n",
    "for idx,word_idx in enumerate(y_test):\n",
    "    y_test_array[idx,word_idx] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**T1.5** Building and training the model.\n",
    "\n",
    "Now you can construct your neural network. You should add the Embedding layer as the first layer. To mask any words that do not have a pre-trained embedding (which will be represented as all zeros) you can configure mask_zero = True in the Embedding layer.\n",
    "\n",
    "\n",
    "The model will be very similar to the model from the last practical. Instead of the Convolutional layer you will be using LSTM layer. Please read about different configuration of the LSTM layer in [here](https://keras.io/api/layers/recurrent_layers/lstm/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1293/1293 [==============================] - 73s 55ms/step - loss: 7.4518 - accuracy: 0.0429 - val_loss: 6.8582 - val_accuracy: 0.0513\n",
      "Epoch 2/15\n",
      "1293/1293 [==============================] - 68s 52ms/step - loss: 7.1594 - accuracy: 0.0525 - val_loss: 6.7149 - val_accuracy: 0.0710\n",
      "Epoch 3/15\n",
      "1293/1293 [==============================] - 68s 52ms/step - loss: 6.9991 - accuracy: 0.0651 - val_loss: 6.5775 - val_accuracy: 0.0877\n",
      "Epoch 4/15\n",
      "1293/1293 [==============================] - 67s 52ms/step - loss: 6.8421 - accuracy: 0.0766 - val_loss: 6.4467 - val_accuracy: 0.0967\n",
      "Epoch 5/15\n",
      "1293/1293 [==============================] - 69s 53ms/step - loss: 6.6918 - accuracy: 0.0862 - val_loss: 6.3533 - val_accuracy: 0.1124\n",
      "Epoch 6/15\n",
      "1293/1293 [==============================] - 68s 53ms/step - loss: 6.5651 - accuracy: 0.0948 - val_loss: 6.2977 - val_accuracy: 0.1170\n",
      "Epoch 7/15\n",
      "1293/1293 [==============================] - 68s 53ms/step - loss: 6.4439 - accuracy: 0.0999 - val_loss: 6.2549 - val_accuracy: 0.1203\n",
      "Epoch 8/15\n",
      "1293/1293 [==============================] - 68s 53ms/step - loss: 6.3356 - accuracy: 0.1040 - val_loss: 6.2231 - val_accuracy: 0.1234\n",
      "Epoch 9/15\n",
      "1293/1293 [==============================] - 71s 55ms/step - loss: 6.2224 - accuracy: 0.1068 - val_loss: 6.2146 - val_accuracy: 0.1266\n",
      "Epoch 10/15\n",
      "1293/1293 [==============================] - 69s 54ms/step - loss: 6.1230 - accuracy: 0.1111 - val_loss: 6.2100 - val_accuracy: 0.1268\n",
      "Epoch 11/15\n",
      "1293/1293 [==============================] - 69s 53ms/step - loss: 6.0346 - accuracy: 0.1136 - val_loss: 6.2146 - val_accuracy: 0.1283\n",
      "Epoch 12/15\n",
      "1293/1293 [==============================] - 68s 53ms/step - loss: 5.9478 - accuracy: 0.1150 - val_loss: 6.2545 - val_accuracy: 0.1297\n",
      "Epoch 13/15\n",
      "1293/1293 [==============================] - 68s 53ms/step - loss: 5.8736 - accuracy: 0.1161 - val_loss: 6.2720 - val_accuracy: 0.1306\n",
      "Epoch 14/15\n",
      "1293/1293 [==============================] - 69s 54ms/step - loss: 5.8047 - accuracy: 0.1172 - val_loss: 6.3354 - val_accuracy: 0.1302\n",
      "Epoch 15/15\n",
      "1293/1293 [==============================] - 68s 53ms/step - loss: 5.7374 - accuracy: 0.1198 - val_loss: 6.3659 - val_accuracy: 0.1300\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "              input_length = train_length,\n",
    "              output_dim=300,\n",
    "              weights=[embedding_matrix],\n",
    "              trainable=False,\n",
    "              mask_zero=True))\n",
    "\n",
    "# Recurrent layer\n",
    "model.add(LSTM(64,dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train,  y_train_array, batch_size=64, epochs=15, validation_data=(x_test, y_test_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Sentiment Analysis with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an LSTM Neural Network to solve the sentiment analysis problem from the last practical. You can explore different variants of the models (with pre-trained embeddings, with embeddings trained via the Embedding layer, transfer learning embeddings using word2vec). To avoid the RNN model to be trained on the padded values, you can configure mask_zero = True in the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('yelp_reviews.csv',encoding = \"ISO-8859-1\")\n",
    "\n",
    "#select input and output variables\n",
    "data = df.values[:,0]\n",
    "labels = df.values[:,1]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels,test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing. Encoding each entry from the train/test sets as sequence of integers for the Embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))\n",
    "max(length)\n",
    "\n",
    "num_words = len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_seq = pad_sequences(sequences, maxlen=45)\n",
    "sequences_val = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(sequences_val, maxlen=45)\n",
    "x_test_seq=np.asarray(x_test_seq).astype(np.float32)\n",
    "x_train_seq=np.asarray(x_train_seq).astype(np.float32)\n",
    "y_test=np.asarray(y_test).astype(np.float32)\n",
    "y_train=np.asarray(y_train).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the weight matrix with pre-treined word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(tokenizer.word_index)+1\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_vectors:\n",
    "        embedding_vector = word2vec[word]\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "16/16 - 4s - loss: 0.6790 - accuracy: 0.5442 - val_loss: 0.6498 - val_accuracy: 0.7108 - 4s/epoch - 231ms/step\n",
      "Epoch 2/5\n",
      "16/16 - 1s - loss: 0.5614 - accuracy: 0.7610 - val_loss: 0.5304 - val_accuracy: 0.7430 - 707ms/epoch - 44ms/step\n",
      "Epoch 3/5\n",
      "16/16 - 1s - loss: 0.4445 - accuracy: 0.8092 - val_loss: 0.4610 - val_accuracy: 0.8032 - 666ms/epoch - 42ms/step\n",
      "Epoch 4/5\n",
      "16/16 - 1s - loss: 0.3893 - accuracy: 0.8353 - val_loss: 0.4502 - val_accuracy: 0.7932 - 697ms/epoch - 44ms/step\n",
      "Epoch 5/5\n",
      "16/16 - 1s - loss: 0.3169 - accuracy: 0.8735 - val_loss: 0.4424 - val_accuracy: 0.8032 - 685ms/epoch - 43ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e8ac510a30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "e = Embedding(num_words, 300, weights=[embedding_matrix], input_length=45, trainable=False, mask_zero = True)\n",
    "model.add(e)\n",
    "model.add(LSTM(64, dropout=0.1, recurrent_dropout=0.1))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_seq, y_train, validation_data=(x_test_seq, y_test), epochs=5, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
